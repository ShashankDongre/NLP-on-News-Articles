{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ba8be091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "custom_stopwords=stopwords.words('english')\n",
    "# custom_stopwords.extend(['said','-','like','many','told','can','could','would','should','a','want','will','about', 'actually', 'almost', 'also', 'although', 'always', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'became', 'become', 'but', 'by', 'can', 'could', 'did', 'do', 'does', 'each', 'either', 'else', 'for', 'from', 'had', 'has', 'have', 'hence', 'how', 'i', 'if', 'in', 'is', 'it', 'its', 'just', 'maybe', 'me', 'might', 'mine', 'must', 'my', 'mine', 'must', 'my', 'neither', 'nor', 'not', 'of', 'oh', 'ok', 'when', 'where', 'whereas', 'wherever', 'whenever', 'whether', 'which', 'while', 'who', 'whom', 'whoever', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yes', 'yet', 'you', 'your'])\n",
    "\n",
    "custom_stopwords.extend(['said','also','need','the','year','take','took','got','years','day','days','used','use','month','mr','mrs','ms','since','hence','henceforth','months','say','says','get','time','come','much','know','u','-','like','many','told','can','could','would','should','a','want','will','about', 'actually', 'almost', 'also', 'although', 'always', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'became', 'become', 'but', 'by', 'can', 'could', 'did', 'do', 'does', 'each', 'either', 'else', 'for', 'from', 'had', 'has', 'have', 'hence', 'how', 'i', 'if', 'in', 'is', 'it', 'its', 'just', 'may', 'maybe', 'me', 'might', 'mine', 'must', 'my', 'mine', 'must', 'my', 'neither', 'nor', 'not', 'of', 'oh', 'ok', 'when', 'where', 'whereas', 'wherever', 'whenever', 'whether', 'which', 'while', 'who', 'whom', 'whoever', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yes', 'yet', 'you', 'your'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "974a9f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load CSV file into DataFrame\n",
    "df = pd.read_excel('output-merged.xlsx')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "72178b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "text = df[['Title', 'Content', 'Number of Words', 'Category','Link']]\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f571e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Title', 'Content', 'Number of Words']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c905c2",
   "metadata": {},
   "source": [
    "## GROUPING CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "92e9b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_categories = [\"football\", \"tennis\", \"sport\", \"motorsport\", \"golf\",\"cricket\",\"hockey\"]\n",
    "country_categories =[\"us\",\"uk\",\"europe\",\"australia\",\"americas\",\"africa\",\"asia\",\"china\",\"india\",\"middleeast\",\"weather\"]\n",
    "tech=[\"tech\",\"technology\"]\n",
    "celeb=[\"media\",\"celebrities\",\"entertainment\"]\n",
    "business=[\"business\",\"Business\",\"homes\",\"success\",\"cars\"]\n",
    "health=[\"health\",\"disability\",\"newsbeat\"]\n",
    "edu=[\"education\",\"explainers\"]\n",
    "pol=[\"politics\",\"opinions\"]\n",
    "travel=[\"NaN\"]\n",
    "science=[\"science\"]\n",
    "money=[\"economy\",\"investing\"]\n",
    "\n",
    "# Define a dictionary to map original category values to subcategory values\n",
    "subcategories_dict = {\"science\":\"science\",\"NaN\":\"travel\",\"education\":\"education\",\"explainers\":\"explainers\",\"cricket\":\"cricket\",\"tech\":\"tech\",\"technology\":\"technology\",\"business\":\"business\",\"Business\":\"Business\",\"health\":\"health\",\"disability\":\"disability\",\"newsbeat\":\"newsbeat\",\"football\": \"football\", \"tennis\": \"tennis\", \"sport\": \"sport\", \"motorsport\": \"motorsport\", \"golf\": \"golf\",\"us\":\"us\",\"uk\":\"uk\",\"europe\":\"europe\",\"australia\":\"australia\",\"americas\":\"americas\",\"africa\":\"africa\",\"asia\":\"asia\",\"china\":\"china\",\"india\":\"india\",\"middleeast\":\"middleast\",\"hockey\":\"hockey\",\"cricket\":\"cricket\",\"economy\":\"economy\",\"investing\":\"investing\",\"homes\":\"homes\",\"opinions\":\"opinions\",\"politics\":\"politics\",\"success\":\"success\",\"cars\":\"cars\",\"science\":\"science\",\"weather\":\"weather\"}\n",
    "#print(df['Category'])\n",
    "# Update the \"Category\" column and create a new \"SubCategory\" column\n",
    "text.loc[text['Category'].isin(sport_categories), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(sport_categories), 'Category'] = 'SPORTS'\n",
    "text.loc[text['Category'].isin(country_categories), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(country_categories), 'Category'] = 'WORLD NEWS'\n",
    "text.loc[text['Category'].isin(tech), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(tech), 'Category'] = 'TECH'\n",
    "text.loc[text['Category'].isin(celeb), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(celeb), 'Category'] = 'ENTERTAINMENT'\n",
    "text.loc[text['Category'].isin(business), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(business), 'Category'] = 'BUSINESS'\n",
    "text.loc[text['Category'].isin(health), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(health), 'Category'] = 'HEALTHY LIVING'\n",
    "text.loc[text['Category'].isin(edu), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(edu), 'Category'] = 'EDUCATION'\n",
    "text.loc[text['Category'].isin(pol), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(pol), 'Category'] = 'POLITICS'\n",
    "text.loc[text['Category'].isin(travel), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(travel), 'Category'] = 'TRAVEL'\n",
    "text.loc[text['Category'].isin(science), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(science), 'Category'] = 'SCIENCE'\n",
    "text.loc[text['Category'].isin(money), 'SubCategory'] = text['Category']\n",
    "text.loc[text['Category'].isin(money), 'Category'] = 'MONEY'\n",
    "\n",
    "#print(df['Category'].isin(sport_categories))\n",
    "# Map the subcategory values using the subcategories_dict dictionary\n",
    "text['SubCategory'] = text['SubCategory'].map(subcategories_dict)\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "text.to_csv('output-merge.csv', index=False)\n",
    "\n",
    " # Define the desired categories\n",
    "desired_categories = ['SPORTS', 'WORLD NEWS', 'TECH', 'ENTERTAINMENT', 'BUSINESS', 'HEALTHY LIVING', 'EDUCATION', 'POLITICS', 'TRAVEL', 'SCIENCE', 'MONEY']\n",
    "\n",
    "# Create a new DataFrame with only the desired categories\n",
    "text_filtered = text[text['Category'].isin(desired_categories)]\n",
    "\n",
    "# Save the filtered DataFrame to a CSV file\n",
    "text_filtered.to_csv('output-filtered.csv', index=False)\n",
    "\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507125d",
   "metadata": {},
   "source": [
    "## Replace empty \"Content\" with corresponding \"Title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "40e9b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "text['Content'] = text['Content'].replace(' ', np.nan)\n",
    "text['Content'].fillna(text['Title'], inplace=True)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5ad80",
   "metadata": {},
   "source": [
    "## Word count for the \"Content\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cb5fe05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(cell):\n",
    "    words = cell.split()\n",
    "    return len(words)\n",
    "\n",
    "# Apply the count_words function to the 'Content' column using apply() and lambda\n",
    "text['Word Count(new)'] = text['Content'].apply(lambda x: count_words(x))\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9427517",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "There is no missing data, therefore, we can move to the next stage. For Term frequency analysis, it is essential that the text data be preprocessed.\n",
    "\n",
    "* Lowercase\n",
    "* Remove punctutations\n",
    "* Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "53845ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(content):\n",
    "    \n",
    "    content = content.lower()\n",
    "    content = re.sub('[^a-z A-Z 0-9-]+', '', content)\n",
    "    content = \" \".join([word for word in content.split() if word not in custom_stopwords])\n",
    "    content = content.replace(\"\\n\", \"\").strip()\n",
    "    return content\n",
    "\n",
    "def tclean(title):\n",
    "    \n",
    "    title = title.replace(\"\\n\", \"\").strip()\n",
    "    return title\n",
    "\n",
    "text['Content'] = text['Content'].apply(clean)\n",
    "text['Title'] = text['Title'].apply(clean)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0d9b6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "text['Word Count(new)'] = text['Content'].apply(lambda x: count_words(x))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eee3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(text):\n",
    "    text_list = text.split()\n",
    "    return text_list\n",
    "\n",
    "text['Content_list'] = text['Content'].apply(corpus)\n",
    "df['Content_list']=df['Content'].apply(corpus)\n",
    "\n",
    "text.to_csv('cleaned.csv')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9607f7",
   "metadata": {},
   "source": [
    "## Word frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5ab66",
   "metadata": {},
   "source": [
    "Word frequency analysis is a technique used to determine the frequency of each word in a given text or corpus. \n",
    "\n",
    "In Python, the Natural Language Toolkit (NLTK) library provides a range of tools for text analysis, including word frequency analysis. \n",
    "\n",
    "NLTK's word frequency analysis capabilities involve counting the occurrence of each word in a given text and then generating a frequency distribution table that lists each word and its frequency in descending order. \n",
    "\n",
    "NLTK's frequency distribution table can be used to identify the most common words in a text, which can provide insights into the topic or theme of the text. \n",
    "\n",
    "NLTK's word frequency analysis is a useful tool for a variety of applications, such as content analysis, topic modeling, and keyword research. \n",
    "\n",
    "Additionally, NLTK provides the ability to customize the word frequency analysis by removing stopwords, punctuation, and other noise words to obtain a more accurate representation of the frequency of meaningful words in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "55dac897",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ''.join([word for word in df['Content']]).replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6983b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=1000, height=700, max_font_size=110, background_color=\"black\",colormap='rainbow', max_words=1000, contour_width=3, contour_color='red',collocations=False,stopwords=custom_stopwords)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(all_words)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4ab1234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import trange\n",
    "\n",
    "corpus = []\n",
    "for i in trange(text1.shape[0], ncols=150, nrows=10, colour='green', smoothing=0.8):\n",
    "    corpus += text['Content_list'][i]\n",
    "len(corpus)\n",
    "\n",
    "mostCommon = Counter(corpus).most_common(15)\n",
    "mostCommon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16cb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "freq = []\n",
    "for word, count in mostCommon:\n",
    "    words.append(word)\n",
    "    freq.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x=freq, y=words)\n",
    "plt.title('Top 15 Most Frequently Occuring Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c721912",
   "metadata": {},
   "source": [
    "> A stat that shows most news is \"he said, she said\"! XD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd648c71",
   "metadata": {},
   "source": [
    "## Most Frequently occuring N_grams\n",
    "\n",
    "An n-gram is sequence of n words in a text. Most words by themselves may not present the entire context. Typically adverbs such as 'most' or 'very' are used to modify verbs and adjectives. Therefore, n-grams help analyse phrases and not just words which can lead to better insights.\n",
    "\n",
    "A Bi-gram means two words in a sequence. 'Very good' or 'Too great'\n",
    "A Tri-gram means three words in a sequence. 'How was your day' would be broken down to 'How was your' and 'was your day'.\n",
    "\n",
    "For separating text into n-grams, we will use *CountVectorizer* from Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b15aba",
   "metadata": {},
   "source": [
    "<h2><b>BIGRAMS</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefcfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "bigrams = cv.fit_transform(text['Content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9ef1c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_values = bigrams.toarray().sum(axis=0)\n",
    "ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse = True))\n",
    "ngram_freq.columns = [\"frequency\", \"bi-gram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=ngram_freq['frequency'][:15], y=ngram_freq['bi-gram'][:15])\n",
    "plt.title('Top 10 Most Frequently Occuring Bigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169579a1",
   "metadata": {},
   "source": [
    "<h2><b>TRIGRAMS</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(ngram_range=(3,3))\n",
    "trigrams = cv1.fit_transform(text['Content'])\n",
    "count_values = trigrams.toarray().sum(axis=0)\n",
    "ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv1.vocabulary_.items()], reverse = True))\n",
    "ngram_freq.columns = [\"frequency\", \"Tri-gram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0de4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=ngram_freq['frequency'][:15], y=ngram_freq['Tri-gram'][:15])\n",
    "plt.title('Top 10 Most Frequently Occuring Trigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630c9a9",
   "metadata": {},
   "source": [
    "First, Weâ€™ll take a look at the number of characters present in each sentence. This can give us a rough idea about the news headline length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc955473",
   "metadata": {},
   "outputs": [],
   "source": [
    "text['Title'].str.len().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ddcec",
   "metadata": {},
   "source": [
    "Each headline has about 100 characters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text['Title'].str.split().map(lambda x: len(x)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c57fa1",
   "metadata": {},
   "source": [
    "Each headline has about 8 words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711c802",
   "metadata": {},
   "source": [
    "<h1><b>Latest News per day per country Visualization</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all country names\n",
    "countries = {country.name for country in pycountry.countries}\n",
    "# Check for country names in the text\n",
    "for text in text['Title']:\n",
    "    for word in text.split():\n",
    "        if word in countries:\n",
    "            print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba61398",
   "metadata": {},
   "source": [
    "<h2><b>Chloropleth Maps</h2><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f256e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pycountry\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8210a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a set of all country names\n",
    "countries = {country.name for country in pycountry.countries}\n",
    "# Count the number of occurrences of each country in the 'Title' column\n",
    "counts = text['Title'].apply(lambda x: pd.Series([word for word in x.split() if word in countries]))\\\n",
    "                    .stack()\\\n",
    "                    .reset_index(drop=True)\\\n",
    "                    .value_counts()\\\n",
    "                    .reset_index()\\\n",
    "                    .rename(columns={'index': 'country', 0: 'count'})\n",
    "print(counts)\n",
    "# Get the alpha-3 codes for each country\n",
    "def get_alpha3(country_name):\n",
    "    try:\n",
    "        for country in pycountry.countries:\n",
    "           return pycountry.countries.search_fuzzy(country_name)[0].alpha_3\n",
    "    except:\n",
    "        return None\n",
    "counts['alpha_3'] = counts['country'].apply(get_alpha3)\n",
    "\n",
    "# Read the world map shapefile\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world['alpha_3'] = world['iso_a3']\n",
    "world['count'] = counts['count']\n",
    "\n",
    "# Merge the world map with the counts DataFrame\n",
    "merged = world.merge(counts[['alpha_3', 'country']], on='alpha_3', how='left')\n",
    "\n",
    "# # Generate the chloropleth map\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# merged.plot(column='name', cmap='Blues', legend=False, ax=ax, edgecolor='grey', linewidth=0.5, k=5, figsize=(10, 6))\n",
    "# ax.set_title('Country Counts')\n",
    "# ax.set_axis_off()\n",
    "# plt.show()\n",
    "merged.to_csv('country.csv')\n",
    "counts.to_csv('count.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434426e8",
   "metadata": {},
   "source": [
    "<h2><b>More Detailed visualization using Chloropleth Maps</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead776c4",
   "metadata": {},
   "source": [
    "<h3>used bokeh library to do visualization of the news counts per country</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from bokeh.io import show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GeoJSONDataSource, HoverTool\n",
    "from bokeh.palettes import Blues,Plasma,Purples,Blues256\n",
    "from bokeh.models import LinearColorMapper\n",
    "from bokeh.palettes import Magma,Viridis,Inferno,Cividis,Spectral,Category10,Category20,Pastel2\n",
    "\n",
    "# Create a set of all country names\n",
    "countries = {country.name for country in pycountry.countries}\n",
    "# print(countries)\n",
    "# Count the number of occurrences of each country in the 'Title' column\n",
    "counts = text['Title'].apply(lambda x: pd.Series([word for word in x.split() if word in countries]))\\\n",
    "                    .stack()\\\n",
    "                    .reset_index(drop=True)\\\n",
    "                    .value_counts()\\\n",
    "                    .reset_index()\\\n",
    "                    .rename(columns={'index': 'country', 0: 'count'})\n",
    "\n",
    "\n",
    "\n",
    "# Get the alpha-3 codes for each country\n",
    "def get_alpha3(country_name):\n",
    "    if country_name == 'US' or country_name == 'USA' or country_name == 'America' or country_name=='United States':\n",
    "        return pycountry.countries('U.S.')\n",
    "    try:\n",
    "        for country in pycountry.countries:\n",
    "           return pycountry.countries.search_fuzzy(country_name)[0].alpha_3\n",
    "    except:\n",
    "        return None\n",
    "counts['alpha_3'] = counts['country'].apply(get_alpha3)\n",
    "\n",
    "# Read the world map shapefile\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world['alpha_3'] = world['iso_a3']\n",
    "\n",
    "# Merge the world map with the counts DataFrame\n",
    "merged = world.merge(counts[['alpha_3', 'country', 'count']], on='alpha_3', how='left')\n",
    "\n",
    "# Convert merged DataFrame to GeoJSONDataSource\n",
    "geosource = GeoJSONDataSource(geojson=merged.to_json())\n",
    "low = max(counts['count'])\n",
    "high = min(counts['count'])\n",
    "# Define color palette\n",
    "palette = Blues256[::-1]\n",
    "# Instantiate the figure object\n",
    "# p = figure(title='Country Counts', plot_height=600, plot_width=950, toolbar_location=None, tools=[])\n",
    "p = figure(plot_height=840, plot_width=1500, toolbar_location=None, tools=[],background='white',\n",
    "           background_fill_color='black')\n",
    "\n",
    "\n",
    "# Add the polygons to the map\n",
    "p.patches('xs', 'ys', source=geosource, fill_color={'field': 'count', 'transform': LinearColorMapper(palette=palette)}, line_color='white', line_width=0.5, fill_alpha=1)\n",
    "\n",
    "# Add the hover tool\n",
    "hover = HoverTool(tooltips=[('Country', '@name'), ('Count', '@count')], mode='mouse')\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Set the map properties\n",
    "p.xgrid.grid_line_color = None\n",
    "p.ygrid.grid_line_color = None\n",
    "p.axis.visible = False\n",
    "p.outline_line_color = None\n",
    "\n",
    "# Show the map\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b66ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "940ecea5b92da9d4010cf6e861aa27b41957a7e859d7a006d57c019532547302"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
