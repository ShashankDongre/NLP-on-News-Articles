{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5868613a",
   "metadata": {},
   "source": [
    "# News Scraping with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28e59e6",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library used for web scraping to extract data from HTML and XML files. \n",
    "\n",
    "It can be used for news scraping to extract specific tags and their contents such as headline, author, date, and article text. \n",
    "\n",
    "Regular expressions can be used to clean the data.  \n",
    "\n",
    "Beautiful Soup provides various methods and filters to search for specific HTML tags and their contents, making it easier to extract specific information from the webpage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31590c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fe100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topics to search for\n",
    "topic=['https://www.cnn.com/politics', 'https://www.cnn.com/business', \n",
    "         'https://www.cnn.com/health', 'https://www.cnn.com/entertainment',\n",
    "         'https://www.cnn.com/world', 'https://www.cnn.com/us', 'https://www.cnn.com/us/crime-and-justice', \n",
    "         'https://www.cnn.com/world/africa', 'https://www.cnn.com/world/americas', \n",
    "         'https://www.cnn.com/world/asia', 'https://www.cnn.com/world/australia', 'https://www.cnn.com/world/china', \n",
    "         'https://www.cnn.com/world/europe', 'https://www.cnn.com/opinions', \n",
    "         'https://www.cnn.com/world/india']\n",
    "# Open the CSV file for writing\n",
    "with open('news-cnn-updated-1.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Title', 'Content', 'Number of Words', 'Category','Date Published','Link'])\n",
    "\n",
    "    # Set to store processed URLs\n",
    "    processed_urls = set()\n",
    "\n",
    "    # Loop over the topics\n",
    "    for t in topic:\n",
    "        response = requests.get(t)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # desired section for links in the main search page\n",
    "        #sec = soup.find(\"div\", {\"class\": \"container__headline container_lead-plus-headlines-with-images__headline\"})\n",
    "        urls = []\n",
    "        # concat_links=[]\n",
    "        # number of links inside this part\n",
    "        links = soup.find_all(\"a\", {\"class\": \"container__link container_vertical-strip__link\"})\n",
    "        # container__link container_lead-plus-headlines-with-images__link\n",
    "        # print(f'Number of links found: {len(links)}')\n",
    "        # links = sec.select('div a')\n",
    "        # Loop over the anchor tags and extract the URLs\n",
    "  # Loop over the anchor tags and extract the URLs\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href.startswith('http'):\n",
    "                urls.append(href)\n",
    "            else:\n",
    "                urls.append('https://www.cnn.com' + href)\n",
    "            for url in urls:\n",
    "                if url in processed_urls:\n",
    "                    continue\n",
    "                else:\n",
    "                    processed_urls.add(url)\n",
    "\n",
    "                keywords = []\n",
    "                parsed_url = urlparse(url)\n",
    "                path = parsed_url.path\n",
    "                date_published = path.split('/')[1:4]\n",
    "                date_published = '-'.join(date_published)\n",
    "                keywords_str = path[len(date_published)+1:path.rfind('/')]\n",
    "                keywords = keywords_str.split('/')\n",
    "                category = ''\n",
    "                if len(keywords) > 1:\n",
    "                    category = ''.join(keywords[1])\n",
    "                sub_category = ''\n",
    "                if len(keywords) > 2:\n",
    "                    sub_category = ''.join(keywords[2])\n",
    "                    \n",
    "                # Send a GET request to the URL and extract the article content\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                article = soup.find('div', class_=\"article__content-container\")\n",
    "                if article is None:\n",
    "                    continue\n",
    "                title = soup.find('h1').text\n",
    "                text = ''\n",
    "                div_list = article.find_all('p', class_=\"paragraph inline-placeholder\")\n",
    "                for div in div_list:\n",
    "                    text += div.text.strip() + ' '\n",
    "                num_words = len(text.split())\n",
    "\n",
    "                # Write the row to the CSV file\n",
    "                writer.writerow([title, text, num_words,category, date_published, url])\n",
    "                print(sub_category)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bce747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Define the topics to search for\n",
    "topic=['https://www.cnn.com/politics', 'https://www.cnn.com/business', \n",
    "         'https://www.cnn.com/health', 'https://www.cnn.com/entertainment',\n",
    "         'https://www.cnn.com/world', 'https://www.cnn.com/us', 'https://www.cnn.com/us/crime-and-justice', \n",
    "         'https://www.cnn.com/world/africa', 'https://www.cnn.com/world/americas', \n",
    "         'https://www.cnn.com/world/asia', 'https://www.cnn.com/world/australia', 'https://www.cnn.com/world/china', \n",
    "         'https://www.cnn.com/world/europe', 'https://www.cnn.com/opinions', \n",
    "         'https://www.cnn.com/world/india']\n",
    "# Open the CSV file for writing\n",
    "with open('news-cnn-updated-2.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Title', 'Content', 'Number of Words', 'Category', 'Date Published','Link'])\n",
    "\n",
    "    # Set to store processed URLs\n",
    "    processed_urls = set()\n",
    "\n",
    "    # Loop over the topics\n",
    "    for t in topic:\n",
    "        response = requests.get(t)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # desired section for links in the main search page\n",
    "        #sec = soup.find(\"div\", {\"class\": \"container__headline container_lead-plus-headlines-with-images__headline\"})\n",
    "        urls = []\n",
    "        # concat_links=[]\n",
    "        # number of links inside this part\n",
    "        links = soup.find_all(\"a\", {\"class\": \"container__link container_vertical-strip__link\"})\n",
    "        # container__link container_lead-plus-headlines-with-images__link\n",
    "        # print(f'Number of links found: {len(links)}')\n",
    "        # links = sec.select('div a')\n",
    "        # Loop over the anchor tags and extract the URLs\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href.startswith('http'):\n",
    "                urls.append(href)\n",
    "            else:\n",
    "                urls.append('https://www.cnn.com' + href)\n",
    "            for url in urls:\n",
    "                if url in processed_urls:\n",
    "                    continue\n",
    "                else:\n",
    "                    processed_urls.add(url)\n",
    "\n",
    "                keywords = []\n",
    "                parsed_url = urlparse(url)\n",
    "                path = parsed_url.path\n",
    "                date_published = path.split('/')[1:4]\n",
    "                date_published = '-'.join(date_published)\n",
    "                keywords_str = path[len(date_published)+1:path.rfind('/')]\n",
    "                keywords = keywords_str.split('/')\n",
    "                category = ''\n",
    "                if len(keywords) > 1:\n",
    "                    category = ''.join(keywords[1])\n",
    "                sub_category = ''\n",
    "                if len(keywords) > 2:\n",
    "                    sub_category = ''.join(keywords[2])\n",
    "                    \n",
    "                # Send a GET request to the URL and extract the article content\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                article = soup.find('div', class_=\"article__content-container\")\n",
    "                if article is None:\n",
    "                    continue\n",
    "                title = soup.find('h1').text\n",
    "                text = ''\n",
    "                div_list = article.find_all('p', class_=\"paragraph inline-placeholder\")\n",
    "                for div in div_list:\n",
    "                    text += div.text.strip() + ' '\n",
    "                num_words = len(text.split())\n",
    "\n",
    "                # Write the row to the CSV file\n",
    "                writer.writerow([title, text, num_words,category, date_published, url])\n",
    "                print(sub_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topics to search for\n",
    "topic=['https://www.cnn.com/politics', 'https://www.cnn.com/business', \n",
    "         'https://www.cnn.com/health', 'https://www.cnn.com/entertainment',\n",
    "         'https://www.cnn.com/world', 'https://www.cnn.com/us', 'https://www.cnn.com/us/crime-and-justice', \n",
    "         'https://www.cnn.com/world/africa', 'https://www.cnn.com/world/americas', \n",
    "         'https://www.cnn.com/world/asia', 'https://www.cnn.com/world/australia', 'https://www.cnn.com/world/china', \n",
    "         'https://www.cnn.com/world/europe', 'https://www.cnn.com/opinions', \n",
    "         'https://www.cnn.com/world/india', 'https://www.cnn.com/world/middle-east', \n",
    "         'https://www.cnn.com/world/united-kingdom', 'https://www.cnn.com/politics/joe-biden', \n",
    "         'https://www.cnn.com/business/tech', \n",
    "         'https://www.cnn.com/business/media', 'https://www.cnn.com/business/success',\n",
    "         'https://www.cnn.com/entertainment/movies', 'https://www.cnn.com/entertainment/tv-shows', \n",
    "         'https://www.cnn.com/entertainment/celebrities',\n",
    "         'https://www.cnn.com/sport', 'https://www.cnn.com/sport/football', 'https://www.cnn.com/sport/tennis',\n",
    "         'https://www.cnn.com/sport/golf', 'https://www.cnn.com/sport/motorsport', 'https://www.cnn.com/sport/us-sports',\n",
    "         'https://www.cnn.com/sport/milan-cortina-winter-olympics-2026', 'https://www.cnn.com/sport/climbing']\n",
    "# Open the CSV file for writing\n",
    "with open('news-cnn-updated-3.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Title', 'Content', 'Number of Words', 'Category', 'Date Published','Link'])\n",
    "\n",
    "    # Set to store processed URLs\n",
    "    processed_urls = set()\n",
    "\n",
    "    # Loop over the topics\n",
    "    for t in topic:\n",
    "        response = requests.get(t)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # desired section for links in the main search page\n",
    "        #sec = soup.find(\"div\", {\"class\": \"container__headline container_lead-plus-headlines-with-images__headline\"})\n",
    "        urls = []\n",
    "        # concat_links=[]\n",
    "        # number of links inside this part\n",
    "        links = soup.find_all(\"a\", {\"class\": \"container__link container_lead-plus-headlines-with-images__link\"})\n",
    "         \n",
    "        # print(f'Number of links found: {len(links)}')\n",
    "        # links = sec.select('div a')\n",
    "        # Loop over the anchor tags and extract the URLs\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href.startswith('http'):\n",
    "                urls.append(href)\n",
    "            else:\n",
    "                urls.append('https://www.cnn.com' + href)\n",
    "\n",
    "        # Loop over the URLs and extract the articles\n",
    "        for url in urls:\n",
    "                if url in processed_urls:\n",
    "                    continue\n",
    "                else:\n",
    "                    processed_urls.add(url)\n",
    "\n",
    "                keywords = []\n",
    "                parsed_url = urlparse(url)\n",
    "                path = parsed_url.path\n",
    "                date_published = path.split('/')[1:4]\n",
    "                date_published = '-'.join(date_published)\n",
    "                keywords_str = path[len(date_published)+1:path.rfind('/')]\n",
    "                keywords = keywords_str.split('/')\n",
    "                category = ''\n",
    "                if len(keywords) > 1:\n",
    "                    category = ''.join(keywords[1])\n",
    "                sub_category = ''\n",
    "                if len(keywords) > 2:\n",
    "                    sub_category = ''.join(keywords[2])\n",
    "                    \n",
    "                # Send a GET request to the URL and extract the article content\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                article = soup.find('div', class_=\"article__content-container\")\n",
    "                if article is None:\n",
    "                    continue\n",
    "                title = soup.find('h1').text\n",
    "                text = ''\n",
    "                div_list = article.find_all('p', class_=\"paragraph inline-placeholder\")\n",
    "                for div in div_list:\n",
    "                    text += div.text.strip() + ' '\n",
    "                num_words = len(text.split())\n",
    "\n",
    "                # Write the row to the CSV file\n",
    "                writer.writerow([title, text, num_words,category, date_published, url])\n",
    "#                 print(title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topics to search for\n",
    "topic=['https://www.cnn.com/politics', 'https://www.cnn.com/business', \n",
    "         'https://www.cnn.com/health', 'https://www.cnn.com/entertainment',\n",
    "         'https://www.cnn.com/world', 'https://www.cnn.com/us', 'https://www.cnn.com/us/crime-and-justice', \n",
    "         'https://www.cnn.com/world/africa', 'https://www.cnn.com/world/americas', \n",
    "         'https://www.cnn.com/world/asia', 'https://www.cnn.com/world/australia', 'https://www.cnn.com/world/china', \n",
    "         'https://www.cnn.com/world/europe', 'https://www.cnn.com/opinions', \n",
    "         'https://www.cnn.com/world/india', 'https://www.cnn.com/world/middle-east', \n",
    "         'https://www.cnn.com/world/united-kingdom', 'https://www.cnn.com/politics/joe-biden', \n",
    "         'https://www.cnn.com/business/tech', \n",
    "         'https://www.cnn.com/business/media', 'https://www.cnn.com/business/success',\n",
    "         'https://www.cnn.com/entertainment/movies', 'https://www.cnn.com/entertainment/tv-shows', \n",
    "         'https://www.cnn.com/entertainment/celebrities',\n",
    "         'https://www.cnn.com/sport', 'https://www.cnn.com/sport/football', 'https://www.cnn.com/sport/tennis',\n",
    "         'https://www.cnn.com/sport/golf', 'https://www.cnn.com/sport/motorsport', 'https://www.cnn.com/sport/us-sports',\n",
    "         'https://www.cnn.com/sport/milan-cortina-winter-olympics-2026', 'https://www.cnn.com/sport/climbing']\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open('news-cnn-updated-4.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Title', 'Content', 'Number of Words', 'Category', 'Date Published','Link'])\n",
    "\n",
    "    # Set to store processed URLs\n",
    "    processed_urls = set()\n",
    "\n",
    "    # Loop over the topics\n",
    "    for t in topic:\n",
    "        response = requests.get(t)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # desired section for links in the main search page\n",
    "        #sec = soup.find(\"div\", {\"class\": \"container__headline container_lead-plus-headlines-with-images__headline\"})\n",
    "        urls = []\n",
    "        # concat_links=[]\n",
    "        # number of links inside this part\n",
    "        links = soup.find_all(\"a\", {\"class\": \"container__link container_lead-plus-headlines__link\"})\n",
    "         \n",
    "        # print(f'Number of links found: {len(links)}')\n",
    "        # links = sec.select('div a')\n",
    "        # Loop over the anchor tags and extract the URLs\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href.startswith('http'):\n",
    "                urls.append(href)\n",
    "            else:\n",
    "                urls.append('https://www.cnn.com' + href)\n",
    "\n",
    "        # Loop over the URLs and extract the articles\n",
    "        for url in urls:\n",
    "                if url in processed_urls:\n",
    "                    continue\n",
    "                else:\n",
    "                    processed_urls.add(url)\n",
    "\n",
    "                keywords = []\n",
    "                parsed_url = urlparse(url)\n",
    "                path = parsed_url.path\n",
    "                date_published = path.split('/')[1:4]\n",
    "                date_published = '-'.join(date_published)\n",
    "                keywords_str = path[len(date_published)+1:path.rfind('/')]\n",
    "                keywords = keywords_str.split('/')\n",
    "                category = ''\n",
    "                if len(keywords) > 1:\n",
    "                    category = ''.join(keywords[1])\n",
    "                sub_category = ''\n",
    "                if len(keywords) > 2:\n",
    "                    sub_category = ''.join(keywords[2])\n",
    "                    \n",
    "                # Send a GET request to the URL and extract the article content\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                article = soup.find('div', class_=\"article__content-container\")\n",
    "                if article is None:\n",
    "                    continue\n",
    "                title = soup.find('h1').text\n",
    "                text = ''\n",
    "                div_list = article.find_all('p', class_=\"paragraph inline-placeholder\")\n",
    "                for div in div_list:\n",
    "                    text += div.text.strip() + ' '\n",
    "                num_words = len(text.split())\n",
    "\n",
    "                # Write the row to the CSV file\n",
    "                writer.writerow([title, text, num_words,category, date_published, url])\n",
    "#                 print(category,sub_category)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada98db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topics to search for\n",
    "topic=['https://www.cnn.com/politics', 'https://www.cnn.com/business', \n",
    "         'https://www.cnn.com/health', 'https://www.cnn.com/entertainment',\n",
    "         'https://www.cnn.com/world', 'https://www.cnn.com/us', 'https://www.cnn.com/us/crime-and-justice', \n",
    "         'https://www.cnn.com/world/africa', 'https://www.cnn.com/world/americas', \n",
    "         'https://www.cnn.com/world/asia', 'https://www.cnn.com/world/australia', 'https://www.cnn.com/world/china', \n",
    "         'https://www.cnn.com/world/europe', 'https://www.cnn.com/opinions', \n",
    "         'https://www.cnn.com/world/india', 'https://www.cnn.com/world/middle-east', \n",
    "         'https://www.cnn.com/world/united-kingdom', 'https://www.cnn.com/politics/joe-biden', \n",
    "         'https://www.cnn.com/business/tech', \n",
    "         'https://www.cnn.com/business/media', 'https://www.cnn.com/business/success',\n",
    "         'https://www.cnn.com/entertainment/movies', 'https://www.cnn.com/entertainment/tv-shows', \n",
    "         'https://www.cnn.com/entertainment/celebrities',\n",
    "         'https://www.cnn.com/sport', 'https://www.cnn.com/sport/football', 'https://www.cnn.com/sport/tennis',\n",
    "         'https://www.cnn.com/sport/golf', 'https://www.cnn.com/sport/motorsport', 'https://www.cnn.com/sport/us-sports',\n",
    "         'https://www.cnn.com/sport/milan-cortina-winter-olympics-2026', 'https://www.cnn.com/sport/climbing']\n",
    "# Open the CSV file for writing\n",
    "with open('news-cnn-updated-5.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Title', 'Content', 'Number of Words', 'Category', 'Date Published','Link'])\n",
    "\n",
    "    # Set to store processed URLs\n",
    "    processed_urls = set()\n",
    "\n",
    "    # Loop over the topics\n",
    "    for t in topic:\n",
    "        response = requests.get(t)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # desired section for links in the main search page\n",
    "        #sec = soup.find(\"div\", {\"class\": \"container__headline container_lead-plus-headlines-with-images__headline\"})\n",
    "        urls = []\n",
    "        # concat_links=[]\n",
    "        # number of links inside this part\n",
    "        links = soup.find_all(\"a\", {\"class\": \"container__link container_lead-plus-headlines-with-images__link container_lead-plus-headlines-with-images__left container_lead-plus-headlines-with-images__light\"})\n",
    "         \n",
    "        # print(f'Number of links found: {len(links)}')\n",
    "        # links = sec.select('div a')\n",
    "        # Loop over the anchor tags and extract the URLs\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href.startswith('http'):\n",
    "                urls.append(href)\n",
    "            else:\n",
    "                urls.append('https://www.cnn.com' + href)\n",
    "\n",
    "        # Loop over the URLs and extract the articles\n",
    "        for url in urls:\n",
    "                if url in processed_urls:\n",
    "                    continue\n",
    "                else:\n",
    "                    processed_urls.add(url)\n",
    "\n",
    "                keywords = []\n",
    "                parsed_url = urlparse(url)\n",
    "                path = parsed_url.path\n",
    "                date_published = path.split('/')[1:4]\n",
    "                date_published = '-'.join(date_published)\n",
    "                keywords_str = path[len(date_published)+1:path.rfind('/')]\n",
    "                keywords = keywords_str.split('/')\n",
    "                category = ''\n",
    "                if len(keywords) > 1:\n",
    "                    category = ''.join(keywords[1])\n",
    "                sub_category = ''\n",
    "                if len(keywords) > 2:\n",
    "                    sub_category = ''.join(keywords[2])\n",
    "                    \n",
    "                # Send a GET request to the URL and extract the article content\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                article = soup.find('div', class_=\"article__content-container\")\n",
    "                if article is None:\n",
    "                    continue\n",
    "                title = soup.find('h1').text\n",
    "                text = ''\n",
    "                div_list = article.find_all('p', class_=\"paragraph inline-placeholder\")\n",
    "                for div in div_list:\n",
    "                    text += div.text.strip() + ' '\n",
    "                num_words = len(text.split())\n",
    "\n",
    "                # Write the row to the CSV file\n",
    "                writer.writerow([title, text, num_words,category, date_published, url])\n",
    "#                 print(sub_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38acbf3",
   "metadata": {},
   "source": [
    "## BBC Latest news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fdf3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topics to search for\n",
    "topic = [\n",
    "    'https://www.bbc.com/news/world',\n",
    "    'https://www.bbc.com/news/business',\n",
    "    'https://www.bbc.com/news/politics',\n",
    "    'https://www.bbc.com/news/health',\n",
    "    'https://www.bbc.com/news/education',\n",
    "    'https://www.bbc.com/news/science_and_environment',\n",
    "    'https://www.bbc.com/news/technology',\n",
    "    'https://www.bbc.com/news/entertainment_and_arts',\n",
    "    'https://www.bbc.com/news/world-60525350',\n",
    "    'https://www.bbc.com/news/stories',\n",
    "    # 'https://www.bbc.com/news/in_pictures',\n",
    "    'https://www.bbc.com/news/world/asia',\n",
    "    'https://www.bbc.com/sport'\n",
    "]\n",
    "processed_urls=set()\n",
    "# Open the CSV file for writing\n",
    "with open('bbc-latest-news-section.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Title', 'Content', 'Number of Words', 'Category', 'Date Published','Link'])\n",
    "\n",
    "    # Loop over the topics\n",
    "    for t in topic:\n",
    "            response = requests.get(t)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # Extract all the text from the webpage\n",
    "            # Define the URL and the preferred keyword\n",
    "\n",
    "            # Find the links\n",
    "            links = soup.find_all('a', class_=\"qa-heading-link lx-stream-post__header-link\")\n",
    "\n",
    "            # Loop over the links and extract the articles\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                if href in processed_urls:\n",
    "                # Skip already visited URLs\n",
    "                    continue\n",
    "                processed_urls.add(href)\n",
    "                #print(len(processed_urls))\n",
    "                keywords = []\n",
    "                parsed_url = urlparse(href)\n",
    "                path = parsed_url.path\n",
    "                keyword = path.split(\"/\")[-1].split(\"-\")[:2]\n",
    "                category = ''\n",
    "                if len(keyword) > 1:\n",
    "                    category = ''.join(keyword[0])\n",
    "                sub_category = ''\n",
    "                if len(keyword) > 2:\n",
    "                    sub_category = ''.join(keyword[1:])\n",
    "                    \n",
    "                # Create a pandas DataFrame with the URLs and keywords\n",
    "                data = {\n",
    "                \"keyword\": keywords,\n",
    "                }\n",
    "                df = pd.DataFrame(data)\n",
    "\n",
    "                # Save the DataFrame to a CSV file\n",
    "                df.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "                if not href.startswith('http'):\n",
    "                    href = 'https://www.bbc.com' + href\n",
    "\n",
    "                # Get the article content\n",
    "                response = requests.get(href)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                # Find the article title and text\n",
    "                title = soup.find('h1').text\n",
    "                div_list = soup.find_all('div', class_='ssrcss-11r1m41-RichTextComponentWrapper ep2nwvo0')\n",
    "                # div_list = soup.find_all('', class_='paragraph inline-placeholder')\n",
    "                text = ' '\n",
    "                for div in div_list:\n",
    "                    #text += div.text.strip() + ' '\n",
    "                    # print(statement\n",
    "                    # Loop through all the p elements and get their text\n",
    "                    for p in div.find_all('p'):\n",
    "                        text += p.text.strip() + ' '\n",
    "                        num_words = len(text.split())\n",
    "                   \n",
    "                    \n",
    "                    date_published = soup.find('time', {'data-testid': 'timestamp'})['datetime']\n",
    "\n",
    "                # Write the row to the CSV file\n",
    "                writer.writerow([title,text, num_words,category, date_published, href])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd2f13",
   "metadata": {},
   "source": [
    "## BCC all news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topics to search for\n",
    "topic = [\n",
    "    'https://www.bbc.com/news/world',\n",
    "    'https://www.bbc.com/news/business',\n",
    "    'https://www.bbc.com/news/politics',\n",
    "    'https://www.bbc.com/news/health',\n",
    "    'https://www.bbc.com/news/education',\n",
    "    'https://www.bbc.com/news/science_and_environment',\n",
    "    'https://www.bbc.com/news/technology',\n",
    "    'https://www.bbc.com/news/entertainment_and_arts',\n",
    "    'https://www.bbc.com/news/world-60525350',\n",
    "    'https://www.bbc.com/news/stories',\n",
    "    # 'https://www.bbc.com/news/in_pictures',\n",
    "    'https://www.bbc.com/news/world/asia',\n",
    "    'https://www.bbc.com/sport'\n",
    "]\n",
    "# Open the CSV file for writing\n",
    "visited_urls = set()\n",
    "with open('all-news.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Title', 'Content', 'Number of Words', 'Category', 'Date Published','Link'])\n",
    "\n",
    "    # Loop over the topics\n",
    "    for t in topic:\n",
    "            response = requests.get(t)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # Extract all the text from the webpage\n",
    "            # Define the URL and the preferred keyword\n",
    "\n",
    "            # Find the links\n",
    "            links = soup.find_all('a', class_=\"gs-c-promo-heading gs-o-faux-block-link__overlay-link gel-pica-bold nw-o-link-split__anchor\")\n",
    "            \n",
    "            # Loop over the links and extract the articles\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                if href in processed_urls:\n",
    "                # Skip already visited URLs\n",
    "                    continue\n",
    "                processed_urls.add(href)\n",
    "                #print(len(processed_urls))\n",
    "                keywords = []\n",
    "                parsed_url = urlparse(href)\n",
    "                path = parsed_url.path\n",
    "                keyword = path.split(\"/\")[-1].split(\"-\")[:2]\n",
    "                category = ''\n",
    "                if len(keyword) > 1:\n",
    "                    category = ''.join(keyword[0])\n",
    "                sub_category = ''\n",
    "                if len(keyword) > 2:\n",
    "                    sub_category = ''.join(keyword[1:])\n",
    "                #category=soup.find_all('aria-label')\n",
    "                # Create a pandas DataFrame with the URLs and keywords\n",
    "                data = {\n",
    "                \"keyword\": keywords,\n",
    "                }\n",
    "                df = pd.DataFrame(data)\n",
    "\n",
    "                # Save the DataFrame to a CSV file\n",
    "                df.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "                if not href.startswith('http'):\n",
    "                    href = 'https://www.bbc.com' + href\n",
    "\n",
    "                # Get the article content\n",
    "                response = requests.get(href)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                # Find the article title and text\n",
    "                title = soup.find('h1').text\n",
    "                div_list = soup.find_all('div', class_='ssrcss-11r1m41-RichTextComponentWrapper ep2nwvo0')\n",
    "                # div_list = soup.find_all('', class_='paragraph inline-placeholder')\n",
    "                text = ' '\n",
    "                for div in div_list:\n",
    "                    #text += div.text.strip() + ' '\n",
    "                    # print(statement\n",
    "                    # Loop through all the p elements and get their text\n",
    "                    for p in div.find_all('p'):\n",
    "                        text += p.text.strip() + ' '\n",
    "                        num_words = len(text.split())\n",
    "                    # Check if the content is empty, skip storing the record if it is\n",
    "                    \n",
    "                    \n",
    "                    date_published = soup.find('time', {'data-testid': 'timestamp'})['datetime']\n",
    "\n",
    "                # Write the row to the CSV file\n",
    "                writer.writerow([title,text, num_words,category, date_published, href])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea490d",
   "metadata": {},
   "source": [
    "# Fox news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a94ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topics to search for\n",
    "topic=['https://www.foxnews.com/category/us/true-crime', 'https://www.foxnews.com/us', 'https://www.foxnews.com/politics',\n",
    "      'https://www.foxnews.com/world', 'https://www.foxnews.com/media', 'https://www.foxnews.com/entertainment',\n",
    "      'https://www.foxnews.com/sports', 'https://www.foxnews.com/lifestyle', 'https://www.foxbusiness.com',\n",
    "      'https://www.foxnews.com/science', 'https://www.foxnews.com/health']\n",
    "# Open the CSV file for writing\n",
    "with open('fox-news-1.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Title', 'Content', 'Number of Words', 'Category'])\n",
    "\n",
    "    # Loop over the topics\n",
    "    for t in topic:\n",
    "        response = requests.get(t)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # desired section for links in the main search page\n",
    "        sec = soup.find('main', class_=\"main-content\")\n",
    "        urls = []\n",
    "        if sec is None:\n",
    "            continue\n",
    "        # number of links inside this part\n",
    "        links = sec.select('h4.title a, h3.title a')\n",
    "        # print(f'Number of links found: {len(links)}')\n",
    "\n",
    "        # Loop over the anchor tags and extract the URLs\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href.startswith('http'):\n",
    "                urls.append(href)\n",
    "            elif href.startswith('https://www.foxbusiness.com' + href):\n",
    "                urls.append(href)\n",
    "            else:\n",
    "                urls.append('https://www.foxnews.com' + href)\n",
    "        print(urls)\n",
    "        # Loop over the URLs and extract the articles\n",
    "        for url in urls:\n",
    "            keywords = []\n",
    "            parsed_url = urlparse(url)\n",
    "            path = parsed_url.path\n",
    "            date_published = path.split('/')[1:4]\n",
    "            date_published = '-'.join(date_published)\n",
    "            keyword = path.split(\"/\")[-2]\n",
    "            keywords.append(path[len(date_published)+1:path.rfind('/')])\n",
    "            \n",
    "            # Send a GET request to the URL and extract the article content\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            article = soup.find('div', class_=\"article-body\")\n",
    "            if article is None:\n",
    "                continue\n",
    "            title = soup.find('h1').text\n",
    "            text = ''\n",
    "            div_list = article.find_all('p')\n",
    "            for div in div_list:\n",
    "                text += div.text.strip() + ' '\n",
    "            num_words = len(text.split())\n",
    "\n",
    "            # Write the row to the CSV file\n",
    "            writer.writerow([title, text, num_words, keyword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('news-cnn-updated-1.csv')\n",
    "df2 = pd.read_csv('news-cnn-updated-2.csv')\n",
    "df3 = pd.read_csv('news-cnn-updated-3.csv')\n",
    "df4 = pd.read_csv('news-cnn-updated-4.csv')\n",
    "df5 = pd.read_csv('news-cnn-updated-5.csv')\n",
    "df6 = pd.read_csv('all-news.csv')\n",
    "df7 = pd.read_csv('bbc-latest-news-section.csv')\n",
    "df8 = pd.read_csv('fox-news-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e7f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concatenated = pd.concat([ df2,df1, df3, df4,df5,df6,df7, df8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concatenated.to_csv('output-merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b154c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "940ecea5b92da9d4010cf6e861aa27b41957a7e859d7a006d57c019532547302"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
