{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341ddd71",
   "metadata": {},
   "source": [
    "# Text Summarization with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83048537",
   "metadata": {},
   "source": [
    "Spacy is a Python library that provides various natural language processing (NLP) capabilities, including text summarization. \n",
    "\n",
    "Text summarization involves condensing a longer piece of text into a shorter summary, while retaining the most important information. \n",
    "\n",
    "Spacy's summarization capabilities rely on machine learning algorithms that identify the most important sentences in a text and use them to generate a summary. \n",
    "\n",
    "Spacy's summarization capabilities can be customized by adjusting various parameters, such as the length of the summary and the importance assigned to different types of words and phrases. \n",
    "\n",
    "Text summarization with Spacy can be used in a variety of applications, such as news articles, legal documents, and academic papers, to quickly and efficiently distill important information from longer texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8efcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f1fa00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n      Kids under 6 were increasingly treated...</td>\n",
       "      <td>The outbreak of Covid-19 presented many danger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n      How to exercise when you have a chroni...</td>\n",
       "      <td>Many people struggle to maintain a regular wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n      Australia unveils biggest defense over...</td>\n",
       "      <td>Australia has unveiled a radical shakeup of it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n      Another cheetah has died after relocat...</td>\n",
       "      <td>A cheetah from Africa has died two months afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n      Residents in South Florida condo build...</td>\n",
       "      <td>Residents of a South Florida condo building ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>ChatGPT for health care providers: Can the AI ...</td>\n",
       "      <td>OpenAI CEO Sam Altman said that he was \"a litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>Want to get better sleep? Exercise for this lo...</td>\n",
       "      <td>Get the rest you need with these simple tweaks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>Massachusetts town says Avian Flu detected amo...</td>\n",
       "      <td>Fox News Flash top headlines are here. Check o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>The world lost faith in childhood vaccines dur...</td>\n",
       "      <td>Fox News Flash top headlines are here. Check o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>Smart tips to reduce caregiver stress — here's...</td>\n",
       "      <td>Nearly 15% of caregivers reported having 14 or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1210 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title   \n",
       "0     \\n      Kids under 6 were increasingly treated...  \\\n",
       "1     \\n      How to exercise when you have a chroni...   \n",
       "2     \\n      Australia unveils biggest defense over...   \n",
       "3     \\n      Another cheetah has died after relocat...   \n",
       "4     \\n      Residents in South Florida condo build...   \n",
       "...                                                 ...   \n",
       "1205  ChatGPT for health care providers: Can the AI ...   \n",
       "1206  Want to get better sleep? Exercise for this lo...   \n",
       "1207  Massachusetts town says Avian Flu detected amo...   \n",
       "1208  The world lost faith in childhood vaccines dur...   \n",
       "1209  Smart tips to reduce caregiver stress — here's...   \n",
       "\n",
       "                                                Content  \n",
       "0     The outbreak of Covid-19 presented many danger...  \n",
       "1     Many people struggle to maintain a regular wor...  \n",
       "2     Australia has unveiled a radical shakeup of it...  \n",
       "3     A cheetah from Africa has died two months afte...  \n",
       "4     Residents of a South Florida condo building ha...  \n",
       "...                                                 ...  \n",
       "1205  OpenAI CEO Sam Altman said that he was \"a litt...  \n",
       "1206  Get the rest you need with these simple tweaks...  \n",
       "1207  Fox News Flash top headlines are here. Check o...  \n",
       "1208  Fox News Flash top headlines are here. Check o...  \n",
       "1209  Nearly 15% of caregivers reported having 14 or...  \n",
       "\n",
       "[1210 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV file into DataFrame\n",
    "df = pd.read_csv('output-merged.csv')\n",
    "df = df[['Title', 'Content']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc76bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "from string import punctuation\n",
    "punctuation=punctuation+ '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0850f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yamini\n",
      "[nltk_data]     Manral\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the stopwords corpus (only need to do this once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the stopwords into a set\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1bb2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9808bf7",
   "metadata": {},
   "source": [
    "Another way of doing it by defining a function called textSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f535f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textSummarizer(text, percentage):\n",
    "    \n",
    "    # load the model into spaCy\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # pass the text into the nlp function\n",
    "    doc= nlp(text)\n",
    "    \n",
    "    ## The score of each word is kept in a frequency table\n",
    "    tokens=[token.text for token in doc]\n",
    "    freq_of_word=dict()\n",
    "    \n",
    "    # Text cleaning and vectorization \n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in freq_of_word.keys():\n",
    "                    freq_of_word[word.text] = 1\n",
    "                else:\n",
    "                    freq_of_word[word.text] += 1\n",
    "                    \n",
    "    # Maximum frequency of word\n",
    "    max_freq=max(freq_of_word.values())\n",
    "    \n",
    "    # Normalization of word frequency\n",
    "    for word in freq_of_word.keys():\n",
    "        freq_of_word[word]=freq_of_word[word]/max_freq\n",
    "        \n",
    "    # In this part, each sentence is weighed based on how often it contains the token.\n",
    "    sent_tokens= [sent for sent in doc.sents]\n",
    "    sent_scores = dict()\n",
    "    for sent in sent_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in freq_of_word.keys():\n",
    "                if sent not in sent_scores.keys():                            \n",
    "                    sent_scores[sent]=freq_of_word[word.text.lower()]\n",
    "                else:\n",
    "                    sent_scores[sent]+=freq_of_word[word.text.lower()]\n",
    "    \n",
    "    \n",
    "    len_tokens=int(len(sent_tokens)*percentage)\n",
    "    \n",
    "    # Summary for the sentences with maximum score. Here, each sentence in the list is of spacy.span type\n",
    "    summary = nlargest(n = len_tokens, iterable = sent_scores,key=sent_scores.get)\n",
    "    \n",
    "    # Prepare for final summary\n",
    "    final_summary=[word.text for word in summary]\n",
    "    \n",
    "    #convert to a string\n",
    "    summary=\" \".join(final_summary)\n",
    "    \n",
    "    # Return final summary\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24686c89",
   "metadata": {},
   "source": [
    "## Summarising for each news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Summary'] = df['Content'].apply(lambda x: textSummarizer(x,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
